{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPrBFwa93Wm5z/nj7oYe0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasist1987/HackerRank_Python/blob/master/text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTG5TZ_eV8ku"
      },
      "source": [
        "# Text Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urSa6nX8WC_s"
      },
      "source": [
        "### Importing all requisite libraries\n",
        "#### Implementing BART summarizer<br> Bart uses seq2seq architecture Bi-directional BERT encoder and left to right GPT decoder.<br> Only contractions were removed apart from that no other pre-processing is required for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dChYbc_8fdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d67b7cf-06ec-4328-ed1c-cc5a8666f139"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('all') # one time execution\n",
        "import re\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from attention import AttentionLayer\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "!pip install contractions\n",
        "import contractions\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install transformers\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hrTT65yX2B_"
      },
      "source": [
        "#### downloading data from the google drive location provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "0EfhMNKbBf69",
        "outputId": "5b611a3a-f725-4ab5-e627-364f01a23070"
      },
      "source": [
        "\n",
        "url = 'https://drive.google.com/uc?id=1VGthRzHtBSIO182zMCMiqY-YV-D0mLLG'\n",
        "output = 'test.zip'\n",
        "gdown.download(url, output, quiet=False) \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VGthRzHtBSIO182zMCMiqY-YV-D0mLLG\n",
            "To: /content/test.zip\n",
            "210MB [00:01, 132MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw-RBPYeg0Nm"
      },
      "source": [
        "### Function to handle text bigger than the length BART embedding<br> [Function Source](https://colab.research.google.com/drive/1_BgZXBEYPaHapZFTl7ks9RxTxtwQRt7S#scrollTo=yw-RBPYeg0Nm&line=1&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tcY2YkU6NqZ"
      },
      "source": [
        "\n",
        "def summarize(long_text):\n",
        "  model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "  tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "  # tokenize without truncation\n",
        "  inputs_no_trunc = tokenizer(long_text, max_length=None, return_tensors='pt', truncation=False)\n",
        "\n",
        "  # get batches of tokens corresponding to the exact model_max_length\n",
        "  chunk_start = 0\n",
        "  chunk_end = tokenizer.model_max_length  # == 1024 for Bart\n",
        "  inputs_batch_lst = []\n",
        "  while chunk_start <= len(inputs_no_trunc['input_ids'][0]):\n",
        "      inputs_batch = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end]  # get batch of n tokens\n",
        "      inputs_batch = torch.unsqueeze(inputs_batch, 0)\n",
        "      inputs_batch_lst.append(inputs_batch)\n",
        "      chunk_start += tokenizer.model_max_length  # == 1024 for Bart\n",
        "      chunk_end += tokenizer.model_max_length  # == 1024 for Bart\n",
        "\n",
        "  # generate a summary on each batch\n",
        "  summary_ids_lst = [model.generate(inputs, num_beams=4, max_length=100, early_stopping=True) for inputs in inputs_batch_lst]\n",
        "\n",
        "  # decode the output and join into one string with one paragraph per summary batch\n",
        "  summary_batch_lst = []\n",
        "  for summary_id in summary_ids_lst:\n",
        "      summary_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_id]\n",
        "      summary_batch_lst.append(summary_batch[0])\n",
        "  summary_all = '\\n'.join(summary_batch_lst)\n",
        "  return(summary_all)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFuVyu1zhga1"
      },
      "source": [
        "#### Training data provided checking how close the summarization is.<br> Entire source text was concenated ass one and the \"@highlight\" section which had the summary as one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23HWjJBzOkNy",
        "outputId": "24bc7856-54b8-4376-9d61-c7493a70fd5e"
      },
      "source": [
        "\n",
        "df_train=pd.DataFrame()\n",
        "with zipfile.ZipFile('test.zip') as z:\n",
        "  for filename in z.namelist()[0:50]:\n",
        "    with z.open(filename) as f:\n",
        "      dfs=[]\n",
        "      if(filename.find(\"stories_text_summarization_dataset_train\") != -1):\n",
        "        for lines in f:\n",
        "          # lines=lines.splitlines()\n",
        "          lines=lines.decode(\"utf8\").strip()\n",
        "          dfs.append(lines)\n",
        "        dfs=' '.join(dfs)\n",
        "        if(len(dfs)>1):\n",
        "          x,y = dfs.split('@highlight',1)\n",
        "          x=x.replace(\"U.S\",\"United States\")\n",
        "          y=y.replace(\"U.S\",\"United States\")\n",
        "          y=y.replace('@highlight','')\n",
        "          x=contractions.fix(x)\n",
        "          y=contractions.fix(y)\n",
        "          summarized = summarize(x)\n",
        "          df_temp=pd.DataFrame([x,y,summarized]).T\n",
        "          df_train=df_train.append(df_temp)\n",
        "df_train.columns=['Text','Provided_Summary','Predict_Summary']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2042 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1507 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1570 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHCMgqeriQ4u"
      },
      "source": [
        "#### Testing data summarizing for the entire dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BBImSDTBYTy",
        "outputId": "c3d73b33-06df-403d-c46c-ff20959c0f0b"
      },
      "source": [
        "\n",
        "df_test=pd.DataFrame()\n",
        "with zipfile.ZipFile('test.zip') as z:\n",
        "  for filename in z.namelist():\n",
        "    with z.open(filename) as f:\n",
        "      dfs=[]\n",
        "      if(filename.find(\"stories_text_summarization_dataset_test\") != -1):\n",
        "        for lines in f:\n",
        "          # lines=lines.splitlines()\n",
        "          lines=lines.decode(\"utf8\").strip()\n",
        "          dfs.append(lines)\n",
        "        dfs=' '.join(dfs)\n",
        "        if(len(dfs)>1):\n",
        "          x = dfs.split('@highlight',1)[0]\n",
        "          x=x.replace(\"U.S\",\"United States\")\n",
        "          x=contractions.fix(x)\n",
        "          summarized = summarize(x)\n",
        "          df_temp=pd.DataFrame([x,summarized]).T\n",
        "          df_test=df_test.append(df_temp)\n",
        "\n",
        "df_test.columns=['Text','Predict_Summary']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1576 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Vj22HrkA0hm3",
        "outputId": "ed462976-fc84-4992-dddf-8e30280ce173"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Provided_Summary</th>\n",
              "      <th>Predict_Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEHRAN, Iran (CNN) -- Iranian security forces blamed a privately owned vehicle Thursday for allegedly running into protesters during Sunday's opposition protests on a Shiite Muslim holy day, accor...</td>\n",
              "      <td>Video's posters say it shows Iranian police driving into crowds of demonstrators    Truck drives over person in video, which posters say was recorded Sunday    Seven people were killed in anti-g...</td>\n",
              "      <td>Iranian security forces blame privately owned vehicle for allegedly running into protesters. At least seven people were killed Sunday as anti-government protests turned violent. Iranian government...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New York (CNN) -- More than 1,000 people in New Jersey and New York, many of them adolescent Orthodox Jews, have been sickened with mumps since August, health authorities said Monday.  Orange Coun...</td>\n",
              "      <td>Almost all the cases of mumps involve adolescent Orthodox Jews    Outbreak traced to boy who went to United Kingdom during mumps outbreak    Severe cases of mumps can lead to brain inflammation ...</td>\n",
              "      <td>Orange County, New York, has confirmed 494 cases since early November. Almost all of those infected with the virus are of the Orthodox or Hasidic Jewish population. Mumps outbreak began at a summe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(CNN)  -- John Isner of the United States and his French opponent Nicolas Mahut made tennis history at Wimbledon on Wednesday as they contested the longest grand slam singles match of all time -- ...</td>\n",
              "      <td>John Isner and Nicolas Mahut set world record in marathon tennis match at Wimbledon    The match is the longest in terms of time at 10 hours and also games contested    Isner and Mahut tied at 5...</td>\n",
              "      <td>John Isner and Nicolas Mahut play longest grand slam singles match of all time. Match called off at 59 games all in the fifth and deciding set after 10 hours of action on Court 18. Isner has also ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NEW DELHI, India (CNN) -- There was stone cold silence in the car, as the Kumars drove home.  Salwan Montessori School in New Delhi received 2,500 applications -- for 150 spots -- for the 2009 sch...</td>\n",
              "      <td>More Indians moving to urban areas, hence longer waiting lists for nursery schools    Some schools have places for less than 4 percent of applicants    Chosen nursery school determines 14 years ...</td>\n",
              "      <td>Nursery school admission is key to a child's chance at a better life in India. Many students spend 14 years in one school, so where they go to nursery determines the next 14 years of their educati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LONDON, England (CNN) -- A visit to the Eiffel Tower, the Statue of Liberty and the Pyramids is on the itinerary of most jet-setting travelers.  Visiting the Eiffel Tower was voted the most overra...</td>\n",
              "      <td>Some of most iconic global tourist attractions are most disappointing to visit    The Eiffel Tower, the Statue of Liberty and the Pyramids on the list    The Treasury in the ancient city of Petr...</td>\n",
              "      <td>Eiffel Tower, Statue of Liberty and Pyramids voted most disappointing international sights. Big Ben, Buckingham Palace and Princess Diana Memorial Fountain voted most overrated. Alnwick Castle in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                      Text  ...                                                                                                                                                                                          Predict_Summary\n",
              "0  TEHRAN, Iran (CNN) -- Iranian security forces blamed a privately owned vehicle Thursday for allegedly running into protesters during Sunday's opposition protests on a Shiite Muslim holy day, accor...  ...  Iranian security forces blame privately owned vehicle for allegedly running into protesters. At least seven people were killed Sunday as anti-government protests turned violent. Iranian government...\n",
              "0  New York (CNN) -- More than 1,000 people in New Jersey and New York, many of them adolescent Orthodox Jews, have been sickened with mumps since August, health authorities said Monday.  Orange Coun...  ...  Orange County, New York, has confirmed 494 cases since early November. Almost all of those infected with the virus are of the Orthodox or Hasidic Jewish population. Mumps outbreak began at a summe...\n",
              "0  (CNN)  -- John Isner of the United States and his French opponent Nicolas Mahut made tennis history at Wimbledon on Wednesday as they contested the longest grand slam singles match of all time -- ...  ...  John Isner and Nicolas Mahut play longest grand slam singles match of all time. Match called off at 59 games all in the fifth and deciding set after 10 hours of action on Court 18. Isner has also ...\n",
              "0  NEW DELHI, India (CNN) -- There was stone cold silence in the car, as the Kumars drove home.  Salwan Montessori School in New Delhi received 2,500 applications -- for 150 spots -- for the 2009 sch...  ...  Nursery school admission is key to a child's chance at a better life in India. Many students spend 14 years in one school, so where they go to nursery determines the next 14 years of their educati...\n",
              "0  LONDON, England (CNN) -- A visit to the Eiffel Tower, the Statue of Liberty and the Pyramids is on the itinerary of most jet-setting travelers.  Visiting the Eiffel Tower was voted the most overra...  ...  Eiffel Tower, Statue of Liberty and Pyramids voted most disappointing international sights. Big Ben, Buckingham Palace and Princess Diana Memorial Fountain voted most overrated. Alnwick Castle in ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "_oKfBfxvzXh-",
        "outputId": "ed11f1d4-0928-49b6-f705-0103d2f50d7e"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Predict_Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Soon after I first came to visit China in the autumn of 1971, I saw a contingent of militia soldiers doing marching drills in Tiananmen Square. I was told they were rehearsing for the annual Natio...</td>\n",
              "      <td>Mao Zedong formally proclaimed the founding of the People's Republic on October 1, 1949. Mao proved to be good at fighting but poor at governing. He pushed sweeping socio-economic initiatives and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rory McIlroy is off to a good start at the Scottish Open. he is hoping for a good finish, too, after missing the cut at the Irish Open.  McIlroy shot a course record 7-under-par 64 at Royal Aberde...</td>\n",
              "      <td>Rory McIlroy shoots a course record 7-under-par 64 at the Scottish Open. Sweden's Kristoffer Broberg had earlier fired a 65 at Royal Aberdeen. Last year's British Open champion Phil Mickelson is t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The man suspected of trying to attack the Dutch royal family by crashing his car near their bus has died, Dutch police said Friday.  A car is pictured after crashing into the crowd waiting for the...</td>\n",
              "      <td>Police search man's house but find no weapons, explosives. Five people, as well as the driver, were killed and 12 were wounded in the incident. Incident happened during the country's annual Queen'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Twitter is on the verge of its initial public offering and everyone is knickers have been in a knot all week over the company's lack of management diversity -- that is, women. As The New York Time...</td>\n",
              "      <td>Women make up 6% of chief executives at the leading 100 tech companies. Most startups have all-male boards. Women held 16.6% of Fortune 500 board seats in 2012. Women and minorities have lost grou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The White House Wednesday said it was \"puzzled\" by a former spokesman's memoir in which he accuses the Bush administration of being mired in propaganda and political spin and at times playing loos...</td>\n",
              "      <td>NEW: White House \"puzzled\" by memoir by former spokesman Scott McClellan. NEW: Former White House adviser Karl Rove says book sounds like \"left-wing blogger\" NEW: Another former Bush aide-turned-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                      Text                                                                                                                                                                                          Predict_Summary\n",
              "0  Soon after I first came to visit China in the autumn of 1971, I saw a contingent of militia soldiers doing marching drills in Tiananmen Square. I was told they were rehearsing for the annual Natio...  Mao Zedong formally proclaimed the founding of the People's Republic on October 1, 1949. Mao proved to be good at fighting but poor at governing. He pushed sweeping socio-economic initiatives and ...\n",
              "0  Rory McIlroy is off to a good start at the Scottish Open. he is hoping for a good finish, too, after missing the cut at the Irish Open.  McIlroy shot a course record 7-under-par 64 at Royal Aberde...  Rory McIlroy shoots a course record 7-under-par 64 at the Scottish Open. Sweden's Kristoffer Broberg had earlier fired a 65 at Royal Aberdeen. Last year's British Open champion Phil Mickelson is t...\n",
              "0  The man suspected of trying to attack the Dutch royal family by crashing his car near their bus has died, Dutch police said Friday.  A car is pictured after crashing into the crowd waiting for the...  Police search man's house but find no weapons, explosives. Five people, as well as the driver, were killed and 12 were wounded in the incident. Incident happened during the country's annual Queen'...\n",
              "0  Twitter is on the verge of its initial public offering and everyone is knickers have been in a knot all week over the company's lack of management diversity -- that is, women. As The New York Time...  Women make up 6% of chief executives at the leading 100 tech companies. Most startups have all-male boards. Women held 16.6% of Fortune 500 board seats in 2012. Women and minorities have lost grou...\n",
              "0  The White House Wednesday said it was \"puzzled\" by a former spokesman's memoir in which he accuses the Bush administration of being mired in propaganda and political spin and at times playing loos...  NEW: White House \"puzzled\" by memoir by former spokesman Scott McClellan. NEW: Former White House adviser Karl Rove says book sounds like \"left-wing blogger\" NEW: Another former Bush aide-turned-c..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GKsfit-_vXI-",
        "outputId": "6d1ff115-180d-46cf-c2ff-50b844080304"
      },
      "source": [
        "df_test.to_csv('summarization.csv',index=True)\n",
        "from google.colab import files\n",
        "files.download('summarization.csv')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9c471021-947b-4d76-9fa7-5969f6244fdc\", \"summarization.csv\", 45403)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "K4CnJXXRKRfw"
      },
      "source": [
        "#@title\n",
        "sentences = []\n",
        "for s in df_train['Text']:\n",
        "  sentences.append(sent_tokenize(s))\n",
        "sentences_test = []\n",
        "for s in df_test['Text']:\n",
        "  sentences_test.append(sent_tokenize(s))\n",
        "sentences = [y for x in sentences for y in x]\n",
        "sentences_test = [y for x in sentences_test for y in x]\n",
        "\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "clean_sentences_test = pd.Series(sentences_test).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences_test = [s.lower() for s in clean_sentences_test]\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "  return sen_new\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "clean_sentences_test = [remove_stopwords(r.split()) for r in clean_sentences_test]\n",
        "# ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# ! unzip glove*.zip\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)\n",
        "\n",
        "sentence_vectors_test = []\n",
        "for i in clean_sentences_test:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors_test.append(v)\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "sim_mat1 = np.zeros([len(sentences_test), len(sentences_test)])\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "\n",
        "for i in range(len(sentences_test)):\n",
        "  for j in range(len(sentences_test)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors_test[i].reshape(1,100), sentence_vectors_test[j].reshape(1,100))[0,0]\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "nx_graph1 = nx.from_numpy_array(sim_mat1)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "scores_test = nx.pagerank(nx_graph1)\n",
        "\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "ranked_sentences_test = sorted(((scores[i],s) for i,s in enumerate(sentences_test)), reverse=True)\n",
        "sn = 1\n",
        "\n",
        "# Generate summary\n",
        "for i in range(sn):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6E_hz5zq2JV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}